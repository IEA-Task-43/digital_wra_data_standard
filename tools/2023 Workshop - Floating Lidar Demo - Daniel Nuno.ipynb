{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# WRA Data Model User Workshop -\n",
    "# Floating lidar demonstration\n",
    "## by \n",
    "## Daniel Nuno, 29th June 2023\n",
    "\n",
    "</center>\n",
    "\n",
    "This is a notebook demonstrating some of the functionalities of the Task 43 WRA Data Model and how to use it with real floating lidar data. This Notebook was presentated during the workshop on the 29th June 2023. The recording of this is below.\n",
    "\n",
    "https://www.youtube.com/watch?v=MoKDz1FptDA&t=1555s\n",
    "\n",
    "The data used here is publicly available for download at: https://oswbuoysny.resourcepanorama.dnv.com/. Download both the 'E06 Hudson South 10 Minute' and 'E06 Hudson South Hourly' data and save them in the same location as this notebook on your local machine. The 10-min data file contains the lidar measurements and the hourly one contains the ADCP measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to explore Task43 Metadata Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nested_keys(json_data, keys_list):\n",
    "    if isinstance(json_data, dict):\n",
    "        if keys_list and json_data.get(keys_list[0]):\n",
    "            if len(keys_list) == 1:\n",
    "                yield json_data[keys_list[0]]\n",
    "            else:\n",
    "                for result in find_nested_keys(json_data[keys_list[0]], keys_list[1:]):\n",
    "                    yield result\n",
    "    elif isinstance(json_data, list):\n",
    "        for item in json_data:\n",
    "            for result in find_nested_keys(item, keys_list):\n",
    "                yield result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metadata Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List types of measurements\n",
    "fname = '../demo_data/E06_wraMetaData.json' # this file is located in the 'demo_data' folder of the GitHub repository.\n",
    "with open(fname, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing and data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List types of measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measType = list(find_nested_keys(data, [\"measurement_location\", \"measurement_point\", \"measurement_type_id\"]))\n",
    "print(np.unique(measType))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List depths/heights with measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = np.unique(list(find_nested_keys(data, [\"measurement_location\", \"measurement_point\", \"height_m\"])))\n",
    "print(heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Measurement Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measPoints = list(find_nested_keys(data, [\"measurement_location\", \"measurement_point\", \"name\"]))\n",
    "print(len(measPoints))\n",
    "print(measPoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List column names, grouped by Measurement Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varNames = list(find_nested_keys(data, [\"measurement_location\", \"measurement_point\", \"logger_measurement_config\", \"column_name\", \"column_name\"]))\n",
    "varNamesGroup = list(find_nested_keys(data, [\"measurement_location\", \"measurement_point\", \"logger_measurement_config\", \"column_name\"]))\n",
    "print(varNamesGroup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List sensors\n",
    "sensorList = list(find_nested_keys(data, [\"measurement_location\", \"logger_main_config\", \"logger_name\"]))\n",
    "print(sensorList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrity tests for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10min = pd.read_csv('E06_Hudson_South_10_min_avg_20190904_20220327.csv', engine='python').set_index('timestamp').apply(pd.to_numeric, errors='coerce')\n",
    "df10min.index = pd.to_datetime(df10min.index, format='%m-%d-%Y %H:%M')\n",
    "df10min.columns = df10min.columns.str.strip()\n",
    "df1h = pd.read_csv('E06_Hudson_South_hourly_avg_20190904_20220327.csv', engine='python', skiprows=[14322]).set_index('timestamp').apply(pd.to_numeric, errors='coerce') # There is an invalid line in the 1h file, so we skip the 14322 row\n",
    "df1h.index = pd.to_datetime(df1h.index, format='%m-%d-%Y %H:%M')\n",
    "df1h.columns = df1h.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable names in Model but not in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x for x in varNames if (x not in df10min.columns) and (x not in df1h.columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable names in Data but not in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x for x in df10min.columns if x not in varNames])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List column names for wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windSpeedMeasPoints = [x for i, x in enumerate(varNamesGroup) if measType[i] == 'wind_speed']\n",
    "windSpeedNames = [x['column_name'] for sublist in windSpeedMeasPoints for x in sublist if x['statistic_type_id'] == 'avg']\n",
    "lidarWindSpeedNames = [x for x in windSpeedNames if 'LIDAR' in x.upper()]\n",
    "metSpeedNames = [x for x in windSpeedNames if x not in lidarWindSpeedNames]\n",
    "print('Lidar HWS column names: ', lidarWindSpeedNames)\n",
    "print('Meteo HWS column names: ', metSpeedNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze performance of different lidars in campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to plot correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCorrs(df10min):\n",
    "    # Drop rows where either of the two columns is NaN\n",
    "    col1 = lidarWindSpeedNames[0]\n",
    "    col2 = metSpeedNames[0]\n",
    "    df_clean = df10min.dropna(subset=[col1, col2])\n",
    "\n",
    "    # Create a blue scatter plot with a red regression line\n",
    "    sns.regplot(data=df_clean, x=col1, y=col2, color='b', line_kws={'color': 'r'})\n",
    "    plt.title('Scatter plot: Wind speed correlation between first two heights')\n",
    "    plt.xlabel(col1)\n",
    "    plt.ylabel(col2)\n",
    "\n",
    "    # Calculate the correlation coefficient and square it to get R^2\n",
    "    corr_coef = np.corrcoef(df_clean[col1], df_clean[col2])[0,1]\n",
    "    r_squared = corr_coef**2\n",
    "    plt.text(0.1, 0.9, f'RÂ² = {r_squared:.3f}', transform=plt.gca().transAxes)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation for full campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCorrs(df10min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtainin date ranges for each LIDAR and plot Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First identify which loggers are LIDARs\n",
    "lidarLoggers = [i for i, x in enumerate(sensorList) if 'LIDAR' in x.upper()]\n",
    "# get dates for each LIDAR\n",
    "dateRange1 = [pd.to_datetime(list(find_nested_keys(data, [\"measurement_location\", \"logger_main_config\", \"date_from\"]))[lidarLoggers[0]]), pd.to_datetime(list(find_nested_keys(data, [\"measurement_location\", \"logger_main_config\", \"date_to\"]))[lidarLoggers[0]])]\n",
    "dateRange2 = [pd.to_datetime(list(find_nested_keys(data, [\"measurement_location\", \"logger_main_config\", \"date_from\"]))[lidarLoggers[1]]), pd.to_datetime(list(find_nested_keys(data, [\"measurement_location\", \"logger_main_config\", \"date_to\"]))[lidarLoggers[1]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCorrs(df10min[dateRange1[0]:dateRange1[1]])\n",
    "plotCorrs(df10min[dateRange2[0]:dateRange2[1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics for each LIDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Statistics for 1st LIDAR \\n')\n",
    "print(df10min[dateRange1[0]:dateRange1[1]].loc[:,[lidarWindSpeedNames[0], lidarWindSpeedNames[1]]].describe())\n",
    "print('\\n\\n Statistics for 2nd LIDAR \\n')\n",
    "print(df10min[dateRange2[0]:dateRange2[1]].loc[:,[lidarWindSpeedNames[0], lidarWindSpeedNames[1]]].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Availability Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get temporal resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsInDay = int( 24 * 60 / list(find_nested_keys(data, [\"measurement_location\", \"logger_main_config\", \"averaging_period_minutes\"]))[0] )\n",
    "print(pointsInDay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20,10)\n",
    "data = (1-df10min[lidarWindSpeedNames].isna().rolling(pointsInDay).sum().resample('1D').max()/pointsInDay).T\n",
    "sns.heatmap(data, cmap='viridis_r', ax=ax, xticklabels=50)\n",
    "ax.invert_yaxis()\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "labels = ax.set_xticklabels([pd.to_datetime(str(date)).strftime('%Y-%m-%d') for date in data.columns][0::50], rotation=45, ha='right')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
